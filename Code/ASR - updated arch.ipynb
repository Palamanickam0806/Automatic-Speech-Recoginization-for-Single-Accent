{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf9780-f621-474d-a675-9c2bd945d4f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# +-----------------------------------------------------------------------------------------+\n",
    "# | NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
    "# |-----------------------------------------+------------------------+----------------------+\n",
    "# | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "# | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "# |                                         |                        |               MIG M. |\n",
    "# |=========================================+========================+======================|\n",
    "# |   0  NVIDIA GeForce RTX 4080 ...    Off |   00000000:01:00.0 Off |                  N/A |\n",
    "# |  0%   41C    P8              3W /  320W |    7752MiB /  16376MiB |      0%      Default |\n",
    "# |                                         |                        |                  N/A |\n",
    "# +-----------------------------------------+------------------------+----------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f20bc95-9a85-476b-b9f1-e469cb8704d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchaudio.transforms as T\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "import jiwer\n",
    "import editdistance\n",
    "import random\n",
    "import time\n",
    "import heapq\n",
    "import difflib\n",
    "import kenlm\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0713d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_FOLDER = \"wavs\"\n",
    "CSV_PATH = \"data/metadata.csv\"\n",
    "\n",
    "OUTPUT_FOLDER = \"processed_data_aug\"  \n",
    "TRAIN_FOLDER = os.path.join(OUTPUT_FOLDER, \"train\")\n",
    "VAL_FOLDER = os.path.join(OUTPUT_FOLDER, \"val\")\n",
    "TEST_FOLDER = os.path.join(OUTPUT_FOLDER, \"test\")\n",
    "METADATA_FILE_TRAIN = \"train_metadata.csv\"\n",
    "METADATA_FILE_VAL = \"val_metadata.csv\"\n",
    "METADATA_FILE_TEST = \"test_metadata.csv\"\n",
    "WAVS_FILE = 'aug_audio_folder'\n",
    "\n",
    "DEVICE =  torch.device(\"cuda\") #'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "os.makedirs(TRAIN_FOLDER, exist_ok=True)\n",
    "os.makedirs(VAL_FOLDER, exist_ok=True)\n",
    "os.makedirs(TEST_FOLDER, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "transcript_dict = dict(zip(df[\"file_id\"], df[\"transcription\"]))\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333c68e-e6d8-4715-8015-c9c09d94d65f",
   "metadata": {},
   "source": [
    "## **Data Preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5393f788-815b-437c-a5d7-17a06926723d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded\n"
     ]
    }
   ],
   "source": [
    "abbreviation_mapping = {\n",
    "    \"Mr.\": \"Mister\", \"Mrs.\": \"Misess\", \"Dr.\": \"Doctor\", \"No.\": \"Number\", \"St.\": \"Saint\", \"Co.\": \"Company\", \"Jr.\": \"Junior\",\n",
    "    \"Maj.\": \"Major\", \"Gen.\": \"General\", \"Drs.\": \"Doctors\", \"Rev.\": \"Reverend\", \"Lt.\": \"Lieutenant\", \"Hon.\": \"Honorable\",\n",
    "    \"Sgt.\": \"Sergeant\", \"Capt.\": \"Captain\", \"Esq.\": \"Esquire\", \"Ltd.\": \"Limited\", \"Col.\": \"Colonel\", \"Ft.\": \"Fort\"\n",
    "}\n",
    "\n",
    "class AudioAugmentation:\n",
    "    def __init__(self, audio_folder, train_folder, sample_rate=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, wavs_file=WAVS_FILE):\n",
    "        self.audio_folder = audio_folder\n",
    "        self.train_folder = train_folder\n",
    "        self.wavs_file = wavs_file\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.hop_length = hop_length\n",
    "        self.train_metadata_records = []\n",
    "    \n",
    "    def clean_and_expand_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        for abbr, full_form in abbreviation_mapping.items():\n",
    "            text = text.replace(abbr.lower(), full_form.lower())\n",
    "        text = text.replace(\" -- \", \" \")\n",
    "        text = text.replace(\"ü\", \"u\")\n",
    "        text = text.replace(\"etc.\", \"etcetera\")\n",
    "        text = text.replace(\"i.e.\", \"i e \")\n",
    "        text = re.sub(r'[^a-z\\s]', '', text.lower())\n",
    "        return text\n",
    "\n",
    "    def time_stretch(self, y, rate=None):\n",
    "        if rate is None:\n",
    "            rate = random.uniform(0.9, 1.1)\n",
    "        return librosa.effects.time_stretch(y, rate=rate)\n",
    "\n",
    "    def pitch_shift(self, y, sr, n_steps=None):\n",
    "        if n_steps is None:\n",
    "            n_steps = random.randint(-3, 3) \n",
    "        return librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n",
    "\n",
    "    def add_noise(self, y, noise_level=0.005):\n",
    "        noise = np.random.normal(0, noise_level, y.shape)\n",
    "        return y + noise\n",
    "\n",
    "    def apply_specaugment(self, mel_spec_db, warp=False):\n",
    "        time_mask = T.TimeMasking(time_mask_param=20)\n",
    "        freq_mask = T.FrequencyMasking(freq_mask_param=8)\n",
    "        \n",
    "        mel_spec_db = time_mask(torch.tensor(mel_spec_db)).numpy()\n",
    "        mel_spec_db = freq_mask(torch.tensor(mel_spec_db)).numpy()\n",
    "        mel_spec_db[mel_spec_db == 0] = -80.0 \n",
    "        if warp and random.random() < 0.05:\n",
    "            time_warp = T.TimeMasking(time_mask_param=50)\n",
    "            mel_spec_db = time_warp(torch.tensor(mel_spec_db)).numpy()\n",
    "            mel_spec_db[mel_spec_db == 0] = -80.0 \n",
    "        return mel_spec_db\n",
    "    \n",
    "    def save_spectrogram(self, y, sr, file_name):\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.n_mels, hop_length=self.hop_length)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec,ref=1.0)\n",
    "        output_path = os.path.join(self.train_folder, f\"{file_name}.npy\")\n",
    "        np.save(output_path, mel_spec_db)\n",
    "        return output_path\n",
    "    \n",
    "    def augment_and_save(self, y, sr, file_name, aug_type=None):\n",
    "        y_aug = y\n",
    "        if random.random() < 0.6:\n",
    "            if aug_type is None:\n",
    "                aug_type = random.choices([\"stretch\", \"pitch\", \"noise\" , \"mixer\"], weights=[0.20, 0.3, 0.30,0.15], k=1)[0]            \n",
    "            if aug_type == \"stretch\":\n",
    "                y_aug = self.time_stretch(y, random.choice([0.8, 1.2]))\n",
    "            elif aug_type == \"pitch\":\n",
    "                y_aug = self.pitch_shift(y, sr, random.choice([-2, -1, 1, 2]))\n",
    "            elif aug_type == \"noise\":\n",
    "                y_aug = self.add_noise(y, noise_level=random.uniform(0.002, 0.01))\n",
    "            elif aug_type == \"mixer\":\n",
    "                aug_choice = random.choices(\n",
    "                    [\"stretch\", \"pitch\", \"noise\"], \n",
    "                    weights=[0.25, 0.35, 0.25], \n",
    "                    k=2\n",
    "                )        \n",
    "                for mix_aug in aug_choice:\n",
    "                    if mix_aug == \"stretch\":\n",
    "                        y_aug = self.time_stretch(y_aug, random.choice([0.8, 1.2]))  \n",
    "                    elif mix_aug == \"pitch\":\n",
    "                        y_aug = self.pitch_shift(y_aug, sr, random.choice([-2, -1, 1, 2]))  \n",
    "                    elif mix_aug == \"noise\":\n",
    "                        y_aug = self.add_noise(y_aug, noise_level=random.uniform(0.002, 0.01))\n",
    "        else: \n",
    "            aug_type = \"specaugment\"\n",
    "            \n",
    "        wav_output_path = os.path.join(self.wavs_file, f\"{file_name}_{aug_type}.wav\")\n",
    "        sf.write(wav_output_path, y_aug, sr)\n",
    "        \n",
    "        if aug_type == \"specaugment\":\n",
    "            mel_spec = librosa.feature.melspectrogram(y=y_aug, sr=sr, n_mels=self.n_mels, hop_length=self.hop_length)\n",
    "            mel_spec_db_aug = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            mel_spec_db_aug = self.apply_specaugment(mel_spec_db_aug, warp=True)            \n",
    "            aug_output_path = os.path.join(self.train_folder, f\"{file_name}_specaugment.npy\")\n",
    "            np.save(aug_output_path, mel_spec_db_aug)\n",
    "        else:\n",
    "            aug_output_path = self.save_spectrogram(y_aug, sr, f\"{file_name}_{aug_type}\")\n",
    "\n",
    "        self.train_metadata_records.append([file_name + f\"_{aug_type}\", aug_output_path, self.clean_and_expand_text(transcript_dict[file_name])])\n",
    "        return aug_output_path\n",
    "\n",
    "    def process_audio_files(self, train_df, transcript_dict):\n",
    "        for file_id in tqdm(train_df[\"file_id\"], desc=\"Saving original train data\"):\n",
    "            file_path = os.path.join(self.audio_folder, f\"{file_id}.wav\")\n",
    "            file_name = os.path.splitext(file_id)[0]\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    y, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "                    orig_output_path = self.save_spectrogram(y, sr, file_name)\n",
    "                    self.train_metadata_records.append([file_name, orig_output_path, self.clean_and_expand_text(transcript_dict[file_name])])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {e}\")\n",
    "        \n",
    "        # Apply augmentation\n",
    "        for file_id in tqdm(train_df[\"file_id\"], desc=\"Applying augmentation to train data\"):\n",
    "            file_path = os.path.join(self.audio_folder, f\"{file_id}.wav\")\n",
    "            file_name = os.path.splitext(file_id)[0]\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    y, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "                    self.augment_and_save(y, sr, file_name)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "        return self.train_metadata_records\n",
    "    \n",
    "if os.path.exists('processed_data_aug/train'):\n",
    "    print(\"File already downloaded\")\n",
    "else:\n",
    "    train_df = df.sample(frac=0.7, random_state=42)\n",
    "    remain_df = df.drop(train_df.index)\n",
    "\n",
    "    val_df = remain_df.sample(frac=0.5,random_state=42)\n",
    "    test_df = remain_df.drop(val_df.index)\n",
    "\n",
    "    audio_augmentor = AudioAugmentation(AUDIO_FOLDER, TRAIN_FOLDER, SAMPLE_RATE, N_MELS, HOP_LENGTH,WAVS_FILE)\n",
    "    train_metadata_records = audio_augmentor.process_audio_files(train_df, transcript_dict)\n",
    "\n",
    "    train_metadata_df = pd.DataFrame(train_metadata_records, columns=[\"file_id\", \"spectrogram_path\", \"transcript\"])\n",
    "    train_metadata_df.to_csv(METADATA_FILE_TRAIN, index=False)\n",
    "    print(f\"✅ Training data processing complete! Metadata saved in {METADATA_FILE_TRAIN}\")\n",
    "\n",
    "# def max_time_stamps():  \n",
    "#     max_time = 0 \n",
    "#     for file_name in tqdm(os.listdir(trainn_folder), desc=\"Max time stamps computing\"):\n",
    "#         if file_name.endswith(\".npy\"):\n",
    "#             file_path = os.path.join(trainn_folder, file_name)\n",
    "#             mel_spec_db = np.load(file_path)\n",
    "#             time_steps = mel_spec_db.shape[1]\n",
    "#             max_time = max(time_steps,max_time)\n",
    "#     return max_time\n",
    "\n",
    "# def pad_or_truncate_spectrogram(mel_spec_db, max_time):        \n",
    "#     if mel_spec_db.shape[1] < max_time:\n",
    "#         pad_width = max_time - mel_spec_db.shape[1]\n",
    "#         mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, pad_width)), mode='constant',constant_values=-80)\n",
    "#     else:\n",
    "#         mel_spec_db = mel_spec_db[:, :max_time]    \n",
    "#     return mel_spec_db   \n",
    "\n",
    "# trainn_folder = 'processed_data_aug/train' \n",
    "# max_time_steps = max_time_stamps()\n",
    "# print(max_time_steps)\n",
    "\n",
    "# for file_name in tqdm(os.listdir(trainn_folder), desc=\"Padding spectrogram files\"):\n",
    "#     if file_name.endswith(\".npy\"):\n",
    "#         file_path = os.path.join(trainn_folder, file_name)\n",
    "#         mel_spec_db = np.load(file_path)      \n",
    "#         mel_spec_db = pad_or_truncate_spectrogram(mel_spec_db, max_time_steps)\n",
    "#         np.save(file_path, mel_spec_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca8d132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('processed_data_aug/val'):\n",
    "    print(\"File already downloaded\")\n",
    "else:\n",
    "    audio_augmentor = AudioAugmentation(AUDIO_FOLDER, TRAIN_FOLDER, SAMPLE_RATE, N_MELS, HOP_LENGTH)\n",
    "    val_metadata_records = []\n",
    "    for file_id in tqdm(val_df[\"file_id\"], desc=\"Processing validation data\"):\n",
    "        file_path = os.path.join(AUDIO_FOLDER, f\"{file_id}.wav\")\n",
    "        file_name = os.path.splitext(file_id)[0]\n",
    "\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "            val_output_path = os.path.join(VAL_FOLDER, f\"{file_name}.npy\")\n",
    "            np.save(val_output_path, mel_spec_db)\n",
    "    #         val_metadata_records.append([file_name, val_output_path, audio_augmentor.clean_and_expand_text(transcript_dict[file_name])])\n",
    "            cleaned_transcript = audio_augmentor.clean_and_expand_text(transcript_dict.get(file_name, \"No transcript found\"))\n",
    "\n",
    "            val_metadata_records.append([file_name, val_output_path, cleaned_transcript])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    val_metadata_df = pd.DataFrame(val_metadata_records, columns=[\"file_id\", \"spectrogram_path\", \"transcript\"])\n",
    "    val_metadata_df.to_csv(METADATA_FILE_VAL, index=False)\n",
    "    print(f\"✅ Validation data processing complete! Metadata saved in {METADATA_FILE_VAL}\") \n",
    "\n",
    "    test_metadata_records = []\n",
    "    for file_id in tqdm(test_df[\"file_id\"], desc=\"Processing test data\"):\n",
    "        file_path = os.path.join(AUDIO_FOLDER, f\"{file_id}.wav\")\n",
    "        file_name = os.path.splitext(file_id)[0]\n",
    "\n",
    "        try:\n",
    "            y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            test_output_path = os.path.join(TEST_FOLDER, f\"{file_name}.npy\")\n",
    "            np.save(test_output_path, mel_spec_db)\n",
    "            cleaned_transcript = audio_augmentor.clean_and_expand_text(transcript_dict.get(file_name, \"No transcript found\"))\n",
    "\n",
    "            test_metadata_records.append([file_name, test_output_path, cleaned_transcript])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    test_metadata_df = pd.DataFrame(test_metadata_records, columns=[\"file_id\", \"spectrogram_path\", \"transcript\"])\n",
    "    test_metadata_df.to_csv(METADATA_FILE_TEST, index=False)\n",
    "    print(f\"✅ TESTING data processing complete! Metadata saved in {METADATA_FILE_TEST}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bbe7f",
   "metadata": {},
   "source": [
    "# MODEL TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd9ad28-8cec-4bf0-a919-115424b2261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, metadata_df, char_to_idx):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spectrogram_path = self.metadata_df.iloc[idx][\"spectrogram_path\"]\n",
    "        transcript = self.metadata_df.iloc[idx][\"transcript\"]\n",
    "        spectrogram = np.load(spectrogram_path)\n",
    "        encoded_text = [self.char_to_idx[char] for char in transcript if char in self.char_to_idx]\n",
    "        return torch.tensor(spectrogram, dtype=torch.float32), torch.tensor(encoded_text, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    filtered_batch = []  \n",
    "    for spectrogram, text in batch:\n",
    "        spec_length = spectrogram.shape[1]\n",
    "        text_length = len(text)\n",
    "\n",
    "        if text_length == 0:\n",
    "            continue \n",
    "\n",
    "        ratio = spec_length / text_length\n",
    "        if ratio >= 1:\n",
    "            filtered_batch.append((spectrogram, text))\n",
    "\n",
    "    if len(filtered_batch) == 0:\n",
    "        return torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    spectrograms, texts = zip(*filtered_batch)\n",
    "    spectrogram_lengths = torch.tensor([spec.shape[1] for spec in spectrograms])\n",
    "    text_lengths = torch.tensor([len(text) for text in texts])\n",
    "\n",
    "    # Pad spectrograms\n",
    "    max_time_steps = max(spectrogram_lengths)\n",
    "    padded_spectrograms = torch.zeros(len(spectrograms), N_MELS, max_time_steps)\n",
    "    for i, spec in enumerate(spectrograms):\n",
    "        padded_spectrograms[i, :, :spec.shape[1]] = spec\n",
    "\n",
    "    # Pad texts\n",
    "    max_text_length = max(text_lengths)\n",
    "    padded_texts = torch.zeros(len(texts), max_text_length, dtype=torch.long)\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        padded_texts[i, :len(text)] = text\n",
    "\n",
    "    return  padded_spectrograms,spectrogram_lengths, padded_texts, text_lengths\n",
    "def load_arpa_lm(arpa_file):\n",
    "    lm = {}  # Store probabilities with words as string keys\n",
    "    backoff = {}  # Store backoff weights\n",
    "\n",
    "    with open(arpa_file, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()            \n",
    "            if line.startswith(\"\\\\\") or not line:\n",
    "                continue  # Skip headers and empty lines            \n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 2:\n",
    "                prob = float(parts[0])\n",
    "                ngram = parts[1]  # Store as a plain string instead of a tuple                \n",
    "                if len(parts) == 3:  # If there's a backoff weight\n",
    "                    backoff[ngram] = float(parts[2])                \n",
    "                lm[ngram] = prob  # Store LM probability    \n",
    "    return lm, backoff        \n",
    "# def get_lm_score(seq,lm,backoff):\n",
    "#     if seq in lm:\n",
    "#         return lm[seq]\n",
    "#     if len(seq) > 1:\n",
    "#         backoff_word = seq[:-1]\n",
    "#         if backoff_word in backoff:\n",
    "#             return backoff[backoff_word] + get_lm_score(seq[1:], lm, backoff)\n",
    "#     return -10  \n",
    "\n",
    "# def get_best_lm_replacement(word,context, lm, backoff):\n",
    "#     best_score = float('-inf')\n",
    "#     best_word = word  \n",
    "#     closest_match = difflib.get_close_matches(word, lm.keys(), n=3, cutoff=0.8)  # You can adjust cutoff\n",
    "# #     if closest_match:\n",
    "# #         best_word = closest_match[0]\n",
    "# #         best_score = lm[best_word]\n",
    "# #     else:\n",
    "# #         best_word = word\n",
    "#     for candidate in closest_match:\n",
    "#         candidate_ngram = context + \" \" + candidate\n",
    "#         score = get_lm_score(candidate_ngram, lm, backoff)\n",
    "#         if score > best_score:\n",
    "#             best_word = candidate\n",
    "#             best_score = score \n",
    "#     return best_word\n",
    "\n",
    "# def post_process_with_lm(decoded_seq, lm, backoff):\n",
    "#     words = decoded_seq.strip().split()\n",
    "#     final_transcription = []\n",
    "#     for i,word in enumerate(words):\n",
    "#         word = word.strip()\n",
    "#         context = \" \".join(final_transcription[max(0,i-2):i])\n",
    "        \n",
    "#         if word in lm: \n",
    "# #             lm_score = lm[word] \n",
    "#             final_transcription.append(word)  # Keep the word if it's correct\n",
    "#         else:\n",
    "#             # If the word is not in the LM, we look for the best replacement word from the LM\n",
    "#             best_replacement = get_best_lm_replacement(word,context, lm, backoff)\n",
    "#             final_transcription.append(best_replacement)\n",
    "#     return \" \".join(final_transcription)\n",
    "\n",
    "# def beam_search_decoder_lm(probs, char_map, k, lm, backoff):\n",
    "#     T, V = probs.shape\n",
    "#     beam = [(\"\", 0.0)]  # Start with an empty sequence\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         new_beam = {}\n",
    "        \n",
    "#         # Iterate through current beam sequences\n",
    "#         for seq, log_probs in beam:\n",
    "#             # Try appending each possible character to the sequence\n",
    "#             for v in range(V):\n",
    "#                 if v == 0:  # Blank token\n",
    "#                     new_seq = seq  # Keep the sequence unchanged for blank\n",
    "#                 elif len(seq) > 0 and seq[-1] == char_map[v]:  # Avoid repetition of the same character\n",
    "#                     new_seq = seq\n",
    "#                 else:\n",
    "#                     new_seq = seq + char_map[v]\n",
    "                \n",
    "#                 new_probs = log_probs + probs[t, v]  # Update log-probabilities\n",
    "\n",
    "#                 # Store the new sequence with updated probabilities\n",
    "#                 if new_seq in new_beam:\n",
    "#                     new_beam[new_seq] = max(new_beam[new_seq], new_probs)\n",
    "#                 else:\n",
    "#                     new_beam[new_seq] = new_probs\n",
    "        \n",
    "#         # Sort beam by probabilities and keep top k sequences\n",
    "#         beam = heapq.nlargest(k, new_beam.items(), key=lambda x: x[1])\n",
    "    \n",
    "#     # Select the best sequence after beam search\n",
    "#     best_seq = beam[0][0]#'there were more varied and at times especialy when ber had circulated frely more upororious diversiaons' \n",
    "# #     print(f\"Best sequence before LM: {best_seq}\")\n",
    "#     # Join the words with spaces\n",
    "#     final_transcription = post_process_with_lm(best_seq, lm, backoff)\n",
    "    \n",
    "#     return final_transcription\n",
    "# def ctc_decode_lm(output, idx_to_char):    \n",
    "#     pred_indices = output.argmax(dim=-1).cpu().numpy()  \n",
    "#     decoded_text = []\n",
    "    \n",
    "#     for seq in pred_indices:  # Iterate over batch\n",
    "#         chars = []\n",
    "#         prev_char = None  \n",
    "#         for idx in seq:\n",
    "#             if idx != 0 and idx != prev_char:\n",
    "#                 chars.append(idx_to_char[idx])\n",
    "#             prev_char = idx\n",
    "#         decoded_text.append(\"\".join(chars))  \n",
    "#     final_transcription = [post_process_with_lm(text, lm, backoff) for text in decoded_text]\n",
    "#     return final_transcription\n",
    "\n",
    "def ctc_decode(output, idx_to_char):    \n",
    "    pred_indices = output.argmax(dim=-1).cpu().numpy()  \n",
    "    decoded_text = []\n",
    "    \n",
    "    for seq in pred_indices:  # Iterate over batch\n",
    "        chars = []\n",
    "        prev_char = None  \n",
    "        for idx in seq:\n",
    "            if idx != 0 and idx != prev_char:  \n",
    "                chars.append(idx_to_char[idx])\n",
    "            prev_char = idx\n",
    "        decoded_text.append(\"\".join(chars))  \n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede9caa4-440a-403d-83d0-d28267c3ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim , num_layers=4, dropout=0.3,fil_dim=32):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, fil_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(fil_dim),\n",
    "            nn.GELU(),            \n",
    "            nn.Conv2d(fil_dim, fil_dim*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(fil_dim*2),\n",
    "            nn.GELU(),  \n",
    "        )\n",
    "        self.conv1d = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64*input_dim, out_channels = hidden_dim, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.bi_gru = nn.GRU(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        # self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim*2),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim) \n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x.unsqueeze(1) \n",
    "        x = self.conv(x) # (Batch, channels, Features, time) ---- [32, 64, 128, 395]\n",
    "        x = x.permute(0, 3, 2, 1)  # (Batch, channels, Features, time) ---- [32, 64, 128, 395]\n",
    "        x = x.reshape(x.size(0), x.size(1), -1)  # Flatten for RNN input ----- [32, 395, 8192]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.conv1d(x)   # ---- [32, 512, 395]\n",
    "        x = x.permute(0, 2, 1)   # ---- [32, 395, 512]\n",
    "        \n",
    "        lengths = lengths.cpu().int()\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)#---[15264, 256]\n",
    "        packed_output, _ = self.bi_gru(packed_x) #----- [15264, 512]\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True) # ----- [32, 477, 512]\n",
    "        return self.fc(output)\n",
    "        \n",
    "class ASRMetrics:\n",
    "    def __init__(self):\n",
    "        self.cer = []\n",
    "        self.wer = []\n",
    "        \n",
    "    def calculate_cer(self, reference, hypothesis):\n",
    "        return editdistance.eval(reference, hypothesis) / max(len(reference), 1)\n",
    "\n",
    "    def calculate_wer(self, reference, hypothesis):\n",
    "        ref_words = reference.split()\n",
    "        hyp_words = hypothesis.split()\n",
    "        return editdistance.eval(ref_words, hyp_words) / max(len(ref_words), 1)\n",
    "\n",
    "    def log_metrics(self, epoch, predictions, references, writer):\n",
    "        cer_epoch = np.mean([self.calculate_cer(r, p) for r, p in zip(references, predictions)])\n",
    "        wer_epoch = np.mean([self.calculate_wer(r, p) for r, p in zip(references, predictions)])\n",
    "        writer.add_scalar(\"CER\", cer_epoch, epoch)\n",
    "        writer.add_scalar(\"WER\", wer_epoch, epoch)\n",
    "        print(f\"Epoch {epoch+1}: CER: {cer_epoch:.4f}, WER: {wer_epoch:.4f}\")\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b95137f8-49b2-42b0-8698-1d914f3dd474",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_metrics = ASRMetrics()\n",
    "def train_model(model, train_loader, val_loader, epochs=EPOCHS, device=DEVICE):\n",
    "    model.to(device)\n",
    "    criterion = nn.CTCLoss(blank=0) #, reduction='mean', zero_infinity=True\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003) # ,weight_decay=1e-4\n",
    "    early_stopper = EarlyStopping()\n",
    "    writer = SummaryWriter() #log_dir=LOG_DIR\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        first_batch = True\n",
    "        \n",
    "        for spectrograms, spectrogram_lengths, texts, text_lengths in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spectrograms.to(device), spectrogram_lengths.to(device))\n",
    "            outputs = outputs.log_softmax(dim=-1).transpose(0, 1)\n",
    "            loss = criterion(outputs, texts.to(device), spectrogram_lengths.to(device), text_lengths.to(device))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Log gradients & weights\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.requires_grad:\n",
    "            #         writer.add_histogram(f\"Gradients/{name}\", param.grad, epoch)\n",
    "            #         writer.add_histogram(f\"Weights/{name}\", param, epoch)\n",
    "\n",
    "            if first_batch:\n",
    "                writer.add_graph(model, (spectrograms.to(device), spectrogram_lengths.to(device)))\n",
    "                first_batch = False  # Avoid logging graph repeatedly\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.grad is not None:\n",
    "#                     print(f\"Gradient for {name}:\", param.grad.abs().mean().item())            \n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n",
    "\n",
    "# Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        predictions, references = [], []\n",
    "        with torch.no_grad():\n",
    "            for spectrograms, spectrogram_lengths, texts, text_lengths in val_loader:\n",
    "                outputs = model(spectrograms.to(device), spectrogram_lengths.to(device))\n",
    "                outputs = outputs.log_softmax(dim=-1).transpose(0, 1) # (time, batch, vocab)\n",
    "                loss = criterion(outputs, texts.to(device), spectrogram_lengths.to(device), text_lengths.to(device))\n",
    "                val_loss += loss.item()                \n",
    "                outputs = outputs.transpose(0, 1) # correct shape for CER (batch, time, vocab)\n",
    "                b_s = outputs.shape[0]\n",
    "                # decoded_preds = [\"\".join(idx_to_char[idx.item()] for idx in output.argmax(dim=-1) if idx.item() != 0) for output in outputs]\n",
    "                decoded_preds = ctc_decode(outputs, idx_to_char)\n",
    "                decoded_refs = [\"\".join(idx_to_char[idx.item()] for idx in text if idx.item() != 0) for text in texts]\n",
    "#                 print(f\"prediction:{decoded_preds}\")# , reference:{decoded_refs}\n",
    "                predictions.extend(decoded_preds)\n",
    "                references.extend(decoded_refs)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        writer.add_scalar(\"Loss/Validation\", avg_val_loss, epoch)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        asr_metrics.log_metrics(epoch, predictions, references , writer)\n",
    "        \n",
    "        early_stopper(avg_val_loss)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    torch.save(model.state_dict(), \"models/augmented/conv1d/asr_model--4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c084926",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, backoff = load_arpa_lm(\"kenlm/build/3gram.arpa\") \n",
    "train_metadata_df = pd.read_csv(METADATA_FILE_TRAIN)\n",
    "val_metadata_df = pd.read_csv(METADATA_FILE_VAL)\n",
    "test_metadata_df = pd.read_csv(METADATA_FILE_TEST)\n",
    "\n",
    "train_metadata_df[\"transcript\"] = train_metadata_df[\"transcript\"].fillna(\"\").astype(str)\n",
    "val_metadata_df[\"transcript\"] = val_metadata_df[\"transcript\"].fillna(\"\").astype(str)\n",
    "\n",
    "# vocab = sorted(set(\"\".join(train_metadata_df[\"transcript\"] + \"\".join(val_metadata_df[\"transcript\"]))))\n",
    "vocab = [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b6a4e-8d06-474c-9363-5e79b9a96504",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_metadata_df = val_metadata_df.drop_duplicates()\n",
    "train_dataset = ASRDataset(train_metadata_df, char_to_idx)\n",
    "val_dataset = ASRDataset(val_metadata_df, char_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Train model\n",
    "model = ASRModel(N_MELS,512, len(vocab) + 1)\n",
    "train_model(model, train_loader, val_loader, epochs=EPOCHS, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a718ebd",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80736a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lm_score(seq,lm,backoff):\n",
    "    if seq in lm:\n",
    "        return lm[seq]\n",
    "    if len(seq) > 1:\n",
    "        backoff_word = seq[:-1]\n",
    "        if backoff_word in backoff:\n",
    "            return backoff[backoff_word] + get_lm_score(seq[1:], lm, backoff)\n",
    "    return -10         \n",
    "\n",
    "def get_best_lm_replacement(word,context, lm, backoff,thres=2.0):\n",
    "    best_score = float('-inf')\n",
    "    best_word = word  \n",
    "    closest_match = difflib.get_close_matches(word, lm.keys(), n=3, cutoff=0.78)  # You can adjust cutoff\n",
    "#     if closest_match:\n",
    "#         best_word = closest_match[0]\n",
    "#         best_score = lm[best_word]\n",
    "#     else:\n",
    "#         best_word = word\n",
    "    for candidate in closest_match:\n",
    "        candidate_ngram = str(context) + \" \" + candidate\n",
    "        score = get_lm_score(candidate_ngram, lm, backoff)\n",
    "        if score > best_score:\n",
    "            best_word = candidate\n",
    "            best_score = score        \n",
    "    return best_word\n",
    "\n",
    "def post_process_with_lm(decoded_seq, lm, backoff):\n",
    "    words = decoded_seq.strip().split()\n",
    "    final_transcription = []\n",
    "    for i,word in enumerate(words):\n",
    "        word = word.strip()\n",
    "        context = \" \".join(final_transcription[max(0,i-2):i])\n",
    "        \n",
    "        if word in lm: \n",
    "#             lm_score = get_lm_score(context + word,lm,backoff)\n",
    "            final_transcription.append(word)  # Keep the word if it's correct\n",
    "        else:\n",
    "#             If the word is not in the LM, we look for the best replacement word from the LM\n",
    "            best_replacement = get_best_lm_replacement(word,context, lm, backoff)\n",
    "            final_transcription.append(best_replacement) \n",
    "    return \" \".join(final_transcription)\n",
    "\n",
    "def ctc_decode_lm(output, idx_to_char):    \n",
    "    pred_indices = output.argmax(dim=-1).cpu().numpy()  \n",
    "    decoded_text = []\n",
    "    \n",
    "    for seq in pred_indices:  # Iterate over batch\n",
    "        chars = []\n",
    "        prev_char = None  \n",
    "        for idx in seq:\n",
    "            if idx != 0 and idx != prev_char:  \n",
    "                chars.append(idx_to_char[idx])\n",
    "            prev_char = idx\n",
    "        decoded_text.append(\"\".join(chars))  \n",
    "    final_transcription = [post_process_with_lm(text, lm, backoff) for text in decoded_text]\n",
    "    return final_transcription\n",
    "\n",
    "\n",
    "test_metadata_df = pd.read_csv(METADATA_FILE_TEST)\n",
    "test_metadata_df = test_metadata_df.drop_duplicates()\n",
    "test_dataset = ASRDataset(test_metadata_df, char_to_idx)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "predictions, references = [], []\n",
    "epoch = 0\n",
    "writer = SummaryWriter()\n",
    "model = model.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    for spectrograms, spectrogram_lengths, texts, text_lengths in tqdm(test_loader):\n",
    "        outputs = model(spectrograms.to(DEVICE), spectrogram_lengths.to(DEVICE))\n",
    "#         probs = torch.nn.functional.log_softmax(outputs, dim=-1).squeeze().numpy()\n",
    "        outputs = outputs.log_softmax(dim=-1).transpose(0, 1) # (time, batch, vocab)                   \n",
    "        outputs = outputs.transpose(0, 1)  # correct shape for CER (batch, time, vocab)\n",
    "        decoded_preds = ctc_decode_lm(outputs, idx_to_char)\n",
    "        decoded_refs = [\"\".join(idx_to_char[idx.item()] for idx in text if idx.item() != 0) for text in texts] \n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend(decoded_refs)\n",
    "    df_pred = pd.DataFrame({'prediction':predictions,'reference':references})\n",
    "    df_pred.to_csv('pred_vs_ref.csv',index=False,encoding=\"utf-8\")\n",
    "    asr_metrics.log_metrics(epoch, predictions, references , writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe53e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/318 [00:00<?, ?it/s]<timed exec>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 318/318 [02:19<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: CER: 0.0277, WER: 0.0570\n"
     ]
    }
   ],
   "source": [
    "total_data = pd.concat([train_metadata_df,val_metadata_df],ignore_index=True,axis=0)\n",
    "total_data = ASRDataset(total_data, char_to_idx)\n",
    "total_data_loader = DataLoader(total_data, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "predictions, references = [], []\n",
    "epoch = 0\n",
    "writer = SummaryWriter()\n",
    "model = model.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    for spectrograms, spectrogram_lengths, texts, text_lengths in tqdm(total_data_loader):\n",
    "        outputs = model(spectrograms.to(DEVICE), spectrogram_lengths.to(DEVICE))\n",
    "#         probs = torch.nn.functional.log_softmax(outputs, dim=-1).squeeze().numpy()\n",
    "        outputs = outputs.log_softmax(dim=-1).transpose(0, 1) # (time, batch, vocab)                   \n",
    "        outputs = outputs.transpose(0, 1)  # correct shape for CER (batch, time, vocab)\n",
    "        decoded_preds = ctc_decode(outputs, idx_to_char)\n",
    "        decoded_refs = [\"\".join(idx_to_char[idx.item()] for idx in text if idx.item() != 0) for text in texts]       \n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend(decoded_refs)\n",
    "    df_pred = pd.DataFrame({'prediction':predictions,'reference':references})\n",
    "    df_pred.to_csv('pred_ref_mlm.csv',index=False,encoding=\"utf-8\")\n",
    "    asr_metrics.log_metrics(epoch, predictions, references , writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f37754-bdcc-4951-82b7-55085458b0b4",
   "metadata": {},
   "source": [
    "# Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd298668-a62b-4a4f-a5b6-d7fcce1cd6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Trainable\n",
       "============================================================================================================================================\n",
       "ASRModel                                 [1, 128, 486]             [1, 120, 28]              --                        True\n",
       "├─Sequential: 1-1                        [1, 1, 128, 486]          [1, 64, 128, 486]         --                        True\n",
       "│    └─Conv2d: 2-1                       [1, 1, 128, 486]          [1, 32, 128, 486]         320                       True\n",
       "│    └─BatchNorm2d: 2-2                  [1, 32, 128, 486]         [1, 32, 128, 486]         64                        True\n",
       "│    └─GELU: 2-3                         [1, 32, 128, 486]         [1, 32, 128, 486]         --                        --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 128, 486]         [1, 64, 128, 486]         18,496                    True\n",
       "│    └─BatchNorm2d: 2-5                  [1, 64, 128, 486]         [1, 64, 128, 486]         128                       True\n",
       "│    └─GELU: 2-6                         [1, 64, 128, 486]         [1, 64, 128, 486]         --                        --\n",
       "├─Sequential: 1-2                        [1, 8192, 486]            [1, 256, 486]             --                        True\n",
       "│    └─Conv1d: 2-7                       [1, 8192, 486]            [1, 256, 486]             6,291,712                 True\n",
       "│    └─BatchNorm1d: 2-8                  [1, 256, 486]             [1, 256, 486]             512                       True\n",
       "│    └─GELU: 2-9                         [1, 256, 486]             [1, 256, 486]             --                        --\n",
       "├─GRU: 1-3                               [120, 256]                [120, 512]                4,337,664                 True\n",
       "├─Sequential: 1-4                        [1, 120, 512]             [1, 120, 28]              --                        True\n",
       "│    └─LayerNorm: 2-10                   [1, 120, 512]             [1, 120, 512]             1,024                     True\n",
       "│    └─Linear: 2-11                      [1, 120, 512]             [1, 120, 256]             131,328                   True\n",
       "│    └─GELU: 2-12                        [1, 120, 256]             [1, 120, 256]             --                        --\n",
       "│    └─Dropout: 2-13                     [1, 120, 256]             [1, 120, 256]             --                        --\n",
       "│    └─Linear: 2-14                      [1, 120, 256]             [1, 120, 28]              7,196                     True\n",
       "============================================================================================================================================\n",
       "Total params: 10,788,444\n",
       "Trainable params: 10,788,444\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 270.73\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.25\n",
       "Forward/backward pass size (MB): 98.80\n",
       "Params size (MB): 43.15\n",
       "Estimated Total Size (MB): 142.20\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is model summary \n",
    "N_MELS = 128 \n",
    "HIDDEN_DIM = 256\n",
    "VOCAB_SIZE = 28  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ASRModel(N_MELS, HIDDEN_DIM, VOCAB_SIZE).to(device)\n",
    "dummy_input = torch.randn(1, N_MELS, 486).to(device)\n",
    "dummy_lengths = torch.tensor([120]).to(device) #DUMMY_SEQ_LEN\n",
    "\n",
    "summary(model, input_data=(dummy_input, dummy_lengths), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c593ad-07ed-4b0d-8e4f-960b63defab3",
   "metadata": {},
   "source": [
    "# Manual Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08cc071e-412a-4f1b-97dc-483590eeb503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('kenlm'):\n",
    "    print(\"File already downloaded\")\n",
    "else:\n",
    "    text_f = train_metadata_df['transcript'].dropna().unique().tolist() + val_metadata_df['transcript'].dropna().unique().tolist() + test_metadata_df['transcript'].dropna().unique().tolist()\n",
    "    print(len(text_f))\n",
    "\n",
    "    with open('kenlm/build/corpus_full.txt' ,'w', encoding='utf-8') as f:\n",
    "        for line in text_f:\n",
    "            f.write(line.strip() + '\\n')\n",
    "    print('corpus.txt saved sucessfully')\n",
    "# all_trans = pd.concat([train_metadata_df['transcript'],val_metadata_df['transcript']])\n",
    "# word_dict = set()\n",
    "# for transcript in all_trans.dropna():\n",
    "#     words = transcript.lower().split()\n",
    "#     word_dict.update(words)\n",
    "# word_dict = sorted(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70106553-9443-4d5c-92df-71425363e759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: thes a wo icand metsouraiastion energy and wais conperform an wmiol tat charche pardical in ther pat it is unecesary for on o discusions to esamine detailed auntitetif elect rom mavenodicrerlationships a which are defined by maxfwals ecquations a \n",
      "Inference Time: 388.4 ms\n",
      "CPU times: user 7.91 s, sys: 1.1 s, total: 9.01 s\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim , num_layers=4, dropout=0.3,fil_dim=32):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, fil_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(fil_dim),\n",
    "            nn.GELU(),            \n",
    "            nn.Conv2d(fil_dim, fil_dim*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(fil_dim*2),\n",
    "            nn.GELU(),  \n",
    "        )\n",
    "        self.conv1d = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64*input_dim, out_channels = hidden_dim, kernel_size=3, stride=1, padding=1),  \n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.bi_gru = nn.GRU(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        # self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim*2),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim) \n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x.unsqueeze(1) \n",
    "        x = self.conv(x) # (Batch, channels, Features, time) ---- [32, 64, 128, 395]\n",
    "        x = x.permute(0, 3, 2, 1)  # (Batch, channels, Features, time) ---- [32, 64, 128, 395]\n",
    "        x = x.reshape(x.size(0), x.size(1), -1)  # Flatten for RNN input ----- [32, 395, 8192]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.conv1d(x)   # ---- [32, 512, 395]\n",
    "        x = x.permute(0, 2, 1)   # ---- [32, 395, 512]\n",
    "        lengths = torch.tensor(lengths)\n",
    "        lengths = lengths.cpu().int()\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.bi_gru(packed_x)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        return self.fc(output)\n",
    "        \n",
    "# Load the trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def beam_search_decoder(probs,char_map,k):\n",
    "    T,V = probs.shape\n",
    "    beam = [(\"\",0.0)]\n",
    "    for t in range(T):\n",
    "        new_beam={}\n",
    "        for seq,log_probs in beam:\n",
    "            for v in range(V):                \n",
    "                if v == 0:\n",
    "                    new_seq = seq\n",
    "                elif len(seq) > 0 and seq[-1]==char_map[v]:\n",
    "                    new_seq = seq\n",
    "                else:\n",
    "                    new_seq = seq + char_map[v]\n",
    "\n",
    "                new_probs = log_probs + probs[t,v]\n",
    "                \n",
    "                if new_seq in new_beam:\n",
    "                    new_beam[new_seq] = max(new_beam[new_seq], new_probs)\n",
    "                else:\n",
    "                    new_beam[new_seq] = new_probs\n",
    "                   \n",
    "        beam = heapq.nlargest(k,new_beam.items(),key=lambda x:x[1])\n",
    "    return beam[0][0]         \n",
    "    \n",
    "    \n",
    "def transcribe(audio_path, model, char_map, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and process audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    audio = librosa.effects.time_stretch(audio, rate=0.9)\n",
    "#     Audio(audio,rate=16000)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Convert to tensor\n",
    "    audio_tensor = torch.tensor(mel_spectrogram, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        output = model(audio_tensor, [audio_tensor.shape[2]])  # Fix time dimension\n",
    "    probs = torch.nn.functional.log_softmax(output, dim=-1).squeeze().numpy()\n",
    "#     output = output.argmax(dim=-1).squeeze().flatten().tolist()\n",
    "\n",
    "    # Greedy decoding with blank removal\n",
    "    transcription = beam_search_decoder(probs,char_map,k=1)\n",
    "#     prev_char = None\n",
    "#     blank_idx = 0  # Adjust if your blank token index is different\n",
    "    \n",
    "#     for idx in output:\n",
    "#         if idx != prev_char and idx != blank_idx and idx in char_map:\n",
    "#             transcription.append(char_map[idx])\n",
    "#         prev_char = idx  \n",
    "        \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    return \"\".join(transcription), inference_time\n",
    "        \n",
    "\n",
    "# Load the model\n",
    "input_dim = 128 \n",
    "hidden_dim = 512\n",
    "output_dim = 28 \n",
    "model = ASRModel(input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load(\"models/augmented/conv1d/asr_model--3.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Test transcription\n",
    "test_audio = \"aug_audio\"\n",
    "transcription, latency = transcribe(test_audio, model, idx_to_char)\n",
    "\n",
    "\n",
    "print(f\"Transcription: {transcription}\")\n",
    "print(f\"Inference Time: {latency*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca48721",
   "metadata": {},
   "source": [
    "# Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d0f400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/16 [00:00<?, ?it/s]<timed exec>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 16/16 [10:22<00:00, 38.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: CER: 0.0517, WER: 0.1571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_lm_score(seq,lm,backoff):\n",
    "    if seq in lm:\n",
    "        return lm[seq]\n",
    "    if len(seq) > 1:\n",
    "        backoff_word = seq[:-1]\n",
    "        if backoff_word in backoff:\n",
    "            return backoff[backoff_word] + get_lm_score(seq[1:], lm, backoff)\n",
    "    return -10         \n",
    "\n",
    "def get_best_lm_replacement(word,context, lm, backoff,thres=2.0):\n",
    "    best_score = float('-inf')\n",
    "    best_word = word  \n",
    "    closest_match = difflib.get_close_matches(word, lm.keys(), n=1, cutoff=0.85)  # You can adjust cutoff\n",
    "    if closest_match:\n",
    "        best_word = closest_match[0]\n",
    "        best_score = lm[best_word]\n",
    "    else:\n",
    "        best_word = word\n",
    "#     for candidate in closest_match:\n",
    "#         candidate_ngram = str(context) + \" \" + candidate\n",
    "#         score = get_lm_score(candidate_ngram, lm, backoff)\n",
    "#         if score > best_score:\n",
    "#             best_word = candidate\n",
    "#             best_score = score        \n",
    "    return best_word\n",
    "\n",
    "def post_process_with_lm(decoded_seq, lm, backoff):\n",
    "    words = decoded_seq.strip().split()\n",
    "    final_transcription = []\n",
    "    for i,word in enumerate(words):\n",
    "        word = word.strip()\n",
    "        context = \" \".join(final_transcription[max(0,i-2):i])\n",
    "        \n",
    "        if word in lm: \n",
    "#             lm_score = get_lm_score(context + word,lm,backoff)\n",
    "            final_transcription.append(word)  # Keep the word if it's correct\n",
    "        else:\n",
    "#             If the word is not in the LM, we look for the best replacement word from the LM\n",
    "            best_replacement = get_best_lm_replacement(word,context, lm, backoff)\n",
    "            final_transcription.append(best_replacement) \n",
    "    return \" \".join(final_transcription)\n",
    "\n",
    "def ctc_decode_lm(output, idx_to_char):    \n",
    "    pred_indices = output.argmax(dim=-1).cpu().numpy()  \n",
    "    decoded_text = []\n",
    "    \n",
    "    for seq in pred_indices:  # Iterate over batch\n",
    "        chars = []\n",
    "        prev_char = None  \n",
    "        for idx in seq:\n",
    "            if idx != 0 and idx != prev_char:  \n",
    "                chars.append(idx_to_char[idx])\n",
    "            prev_char = idx\n",
    "        decoded_text.append(\"\".join(chars))  \n",
    "    final_transcription = [post_process_with_lm(text, lm, backoff) for text in decoded_text]\n",
    "    return final_transcription\n",
    "\n",
    "\n",
    "test_metadata_df = pd.read_csv(METADATA_FILE_TEST)\n",
    "test_metadata_df = test_metadata_df.drop_duplicates()\n",
    "test_dataset = ASRDataset(test_metadata_df, char_to_idx)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)\n",
    "predictions, references = [], []\n",
    "epoch = 0\n",
    "writer = SummaryWriter()\n",
    "model = model.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    for spectrograms, spectrogram_lengths, texts, text_lengths in tqdm(test_loader):\n",
    "        outputs = model(spectrograms.to(DEVICE), spectrogram_lengths.to(DEVICE))\n",
    "#         probs = torch.nn.functional.log_softmax(outputs, dim=-1).squeeze().numpy()\n",
    "        outputs = outputs.log_softmax(dim=-1).transpose(b0, 1) # (time, batch, vocab)                   \n",
    "        outputs = outputs.transpose(0, 1)  # correct shape for CER (batch, time, vocab)\n",
    "        decoded_preds = ctc_decode_lm(outputs, idx_to_char)\n",
    "        decoded_refs = [\"\".join(idx_to_char[idx.item()] for idx in text if idx.item() != 0) for text in texts] \n",
    "        predictions.extend(decoded_preds)\n",
    "        references.extend(decoded_refs)\n",
    "    df_pred = pd.DataFrame({'prediction':predictions,'reference':references})\n",
    "    df_pred.to_csv('pred_vs_ref.csv',index=False,encoding=\"utf-8\")\n",
    "    asr_metrics.log_metrics(epoch, predictions, references , writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "eef1fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading kenlm/build/3gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "<timed exec>:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript: \n",
      "CPU times: user 5.9 s, sys: 1.07 s, total: 6.97 s\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import heapq\n",
    "import math\n",
    "import difflib\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "# from flashlight.lib.text.dictionary import Dictionary\n",
    "import kenlm\n",
    "\n",
    "# from flashlight.lib.text.decoder import KenLM, LexiconDecoder, LexiconDecoderOptions,CriterionType\n",
    "hotwords = ['pala' , 'vinayak','model' ,'improvement']\n",
    "\n",
    "model = ASRModel(input_dim,hidden_dim,output_dim)\n",
    "model.load_state_dict(torch.load(\"models/augmented/asr_model--11.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# token_dict = Dictionary()\n",
    "# for idx, token in enumerate(vocab):\n",
    "#     token_dict.add_entry(token)\n",
    "# lm = kenlm.Model('kenlm/build/3gram.binary')\n",
    "# lm = KenLM(\"kenlm/build/3gram.binary\" , vocab)\n",
    "LM_WEIGHT = 3.23\n",
    "WORD_SCORE = -0.26\n",
    "\n",
    "beam_search_decoder =ctc_decoder(\n",
    "    lexicon=None,\n",
    "    tokens=vocab,\n",
    "    lm='kenlm/build/3gram.arpa',\n",
    "    nbest=3,\n",
    "    beam_size=1500,\n",
    "    lm_weight=LM_WEIGHT,\n",
    "    word_score=WORD_SCORE,\n",
    "    blank_token=\" \",\n",
    "    sil_token=\" \",\n",
    ")\n",
    "\n",
    "\n",
    "def replace_with_hotwords(sequence, hotwords):\n",
    "    words = sequence.split()\n",
    "    updated_words = []\n",
    "    for word in words:\n",
    "        closest_match = difflib.get_close_matches(word, hotwords, n=1, cutoff=0.75)\n",
    "        if closest_match:\n",
    "            updated_words.append(closest_match[0])  # Replace with hotword\n",
    "        else:\n",
    "            updated_words.append(word)\n",
    "\n",
    "    return \" \".join(updated_words)\n",
    "\n",
    "# def beam_search_decoder(probs, char_map, k, hotwords=None, hotword_weight=10.0):\n",
    "#     if hotwords is None:\n",
    "#         hotwords = []\n",
    "#     hotwords = set(hotwords)\n",
    "#     T, V = probs.shape\n",
    "#     beam = [(\"\", 0.0)]\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         new_beam = {}\n",
    "        \n",
    "#         for seq, log_prob in beam:\n",
    "#             for v in range(V):\n",
    "#                 new_seq = seq\n",
    "                \n",
    "#                 if v != 0:  # Ignore blank index\n",
    "#                     if len(seq) > 0 and seq[-1] == char_map[v]:\n",
    "#                         new_seq = seq  # Repeat character, no addition\n",
    "#                     else:\n",
    "#                         new_seq = seq + char_map[v]\n",
    "                \n",
    "#                 # Adjust probability if the word is a hotword\n",
    "#                 new_prob = log_prob + probs[t, v]\n",
    "                \n",
    "#                 for hotword in hotwords:\n",
    "#                     if new_seq.endswith(hotword):\n",
    "#                         new_prob += hotword_weight\n",
    "# #                     elif hotword.startswith(new_seq):\n",
    "# #                         new_prob += hotword_weight/2\n",
    "# #                     elif hotword in new_seq:\n",
    "# #                         new_prob += hotword_weight/3\n",
    "                \n",
    "#                 # Keep best probability for each sequence\n",
    "#                 if new_seq in new_beam:\n",
    "#                     new_beam[new_seq] = max(new_beam[new_seq], new_prob)\n",
    "#                 else:\n",
    "#                     new_beam[new_seq] = new_prob\n",
    "                   \n",
    "#         # Keep top k sequences\n",
    "#         beam = heapq.nlargest(k, new_beam.items(), key=lambda x: x[1])\n",
    "#         best_sequence = beam[0][0]\n",
    "#         best_sequence = replace_with_hotwords(best_sequence, hotwords)\n",
    "    \n",
    "#     return best_sequence # Return best sequence\n",
    "\n",
    "audio_p = 'testing_audio/into.mp3'\n",
    "audio,sr = librosa.load(audio_p,sr=16000)\n",
    "melspec = librosa.feature.melspectrogram(y=audio,n_mels=128,sr=sr)\n",
    "melspec = librosa.power_to_db(melspec , ref = np.max)\n",
    "audio_tensor = torch.tensor(melspec, dtype = torch.float32).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    output = model(audio_tensor , [audio_tensor.shape[2]])\n",
    "output = torch.nn.functional.log_softmax(output,dim=-1 ).cpu()\n",
    "\n",
    "# emissions = output.squeeze(0)\n",
    "emissions = torch.tensor(output, dtype=torch.float32)\n",
    "# emissions = np.ascontiguousarray(emissions)\n",
    "# T,N = emissions.shape\n",
    "# results = decoder.decode(emissions.ctypes.data_as(np.ctypeslib.as_ctypes(np.float32)), T, N) \n",
    "# best_transcription = results[0].tokens\n",
    "# print(best_transcription)\n",
    "# transcription = beam_search_decoder(output, idx_to_char, k=20, hotwords=hotwords, hotword_weight=10.0)\n",
    "beam_search_result = beam_search_decoder(emissions)\n",
    "beam_search_transcript = \" \".join(beam_search_result[0][0].words).strip()\n",
    "\n",
    "print(f\"Transcript: {beam_search_transcript}\")\n",
    "\n",
    "# decoder = build_ctcdecoder(vocab)\n",
    "# text = decoder.decode(\n",
    "#     output,\n",
    "#     hotwords=hotwords,\n",
    "#     hotword_weight = 10.0,\n",
    "# )\n",
    "# print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b92be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018bc42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1be77b19",
   "metadata": {},
   "source": [
    "#  old methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f71bd-15d6-4df4-951c-8f5095078ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Import your ASR model\n",
    "class ASRModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim , dropout=0.3):\n",
    "        super(ASRModel, self).__init__()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "\n",
    "\n",
    "        # 4-layer Bi-GRU\n",
    "        self.bi_gru = nn.GRU(\n",
    "            input_size=64 * input_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=3,  # Increased to 4 layers\n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        # self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim) \n",
    "        )\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 3, 2, 1)  # (Batch, Time, Features, Channels)\n",
    "        x = x.reshape(x.size(0), x.size(1), -1)  # Flatten for RNN input\n",
    "        lengths = torch.tensor(lengths)\n",
    "        lengths = lengths.cpu().int()\n",
    "        \n",
    "        \n",
    "        # Pack padded sequence for efficiency\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.bi_gru(packed_x)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        return self.fc(output)\n",
    "\n",
    "# Load the trained model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# def transcribe(audio_path, model, char_map, device=\"cpu\"):\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Load and process audio\n",
    "#     audio, sr = librosa.load(audio_path, sr=16000)\n",
    "#     mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
    "#     mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "#     # Convert to tensor\n",
    "#     audio_tensor = torch.tensor(mel_spectrogram, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "#     # Get model output\n",
    "#     with torch.no_grad():\n",
    "#         output = model(audio_tensor, [audio_tensor.shape[1]])\n",
    "#     print(output)\n",
    "#     # Convert to characters\n",
    "#     output = output.argmax(dim=-1).squeeze(0).tolist()\n",
    "#     transcription = \"\".join([char_map[idx] for idx in output if idx in char_map])\n",
    "    \n",
    "#     return transcription\n",
    "def transcribe(audio_path, model, char_map, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and process audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    audio_tensor = torch.tensor(mel_spectrogram, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get model output\n",
    "    with torch.no_grad():\n",
    "        output = model(audio_tensor, [audio_tensor.shape[2]])  # Fix time dimension\n",
    "\n",
    "    # Take argmax and ensure 1D list\n",
    "    output = output.argmax(dim=-1).squeeze().flatten().tolist()\n",
    "\n",
    "    # Greedy decoding with blank removal\n",
    "    transcription = []\n",
    "    prev_char = None\n",
    "    blank_idx = 0  # Adjust if your blank token index is different\n",
    "    \n",
    "    for idx in output:\n",
    "        if idx != prev_char and idx != blank_idx and idx in char_map:\n",
    "            transcription.append(char_map[idx])\n",
    "        prev_char = idx  \n",
    "\n",
    "    return \"\".join(transcription)\n",
    "                      \n",
    "# Load the model\n",
    "input_dim = 128 \n",
    "hidden_dim = 256 \n",
    "output_dim = 28 \n",
    "model = ASRModel(input_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load(\"models/asr_model--8.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Test transcription\n",
    "test_audio = \"wavs/LJ007-0198.wav\"\n",
    "print(\"Transcription:\", transcribe(test_audio, model, idx_to_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec57fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "the windows were to be glazed and painted to prevent prisoners from looking out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a748d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "priting in the only sense with which we are at presene concerned differs fror most ivf not from all the arts incrafts represented in the exivition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd42ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "    \n",
    "LM_WEIGHT = 1.5\n",
    "WORD_SCORE = -0.26\n",
    "\n",
    "beam_search_decoder = ctc_decoder(\n",
    "    lexicon=None, \n",
    "    tokens= vocab,\n",
    "    lm= 'test.arpa',\n",
    "    nbest=3,\n",
    "    beam_size=10,\n",
    "    lm_weight=LM_WEIGHT,\n",
    "    word_score=WORD_SCORE,\n",
    "    blank_token=' ',\n",
    "    sil_token = ' '\n",
    ")\n",
    "\n",
    "\n",
    "model = ASRModel(input_dim,hidden_dim,output_dim)\n",
    "model.load_state_dict(torch.load(\"models/augmented/asr_model--7.pth\"))\n",
    "model.eval()\n",
    "\n",
    "audio_path = 'testing_audio/trouble.mp3'\n",
    "audio , sr = librosa.load(audio_path, sr=16000)\n",
    "mel_spec = librosa.feature.melspectrogram(y=audio,n_mels=128,sr=sr)\n",
    "mel_spec = librosa.power_to_db(mel_spec, ref = np.max)\n",
    "audio_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    output = model(audio_tensor, [audio_tensor.shape[2]])\n",
    "emissions  = torch.nn.functional.log_softmax(output, dim=-1).squeeze()\n",
    "emissions = emissions.unsqueeze(0)\n",
    "result = beam_search_decoder(emissions)\n",
    "\n",
    "token_str = \"\".join(beam_search_decoder.idxs_to_tokens(result[0][0].tokens))\n",
    "transcript = \" \".join(token_str.strip())\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97fb82-1a35-4f23-81a4-9053be7a0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76e017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "def calculate_cer(reference: str, hypothesis: str) -> float:\n",
    "    reference = reference.replace(\" \", \"\")  # CER ignores spaces\n",
    "    hypothesis = hypothesis.replace(\" \", \"\")    \n",
    "    cer = Levenshtein.distance(reference, hypothesis) / max(1, len(reference))\n",
    "    return cer\n",
    "\n",
    "def calculate_wer(reference: str, hypothesis: str) -> float:\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()    \n",
    "    wer = Levenshtein.distance(ref_words, hyp_words) / max(1, len(ref_words))\n",
    "    return wer\n",
    "\n",
    "reference_text = \"We can measure radiation energy and waves can perform and move a charged particle in their path It is unnecessary for quantum discussions to examine detailed quantitative electromagnetic relationships which are defined by Maxwells equations\"\n",
    "hypothesis_text = \"witc an measure radiition energy and weofs con performan wol a charged particle in their pat it is unecessary for qoune of discussions to examine detailed quantity of electrom agneatic relationships  which are defied by maxwallsequations \"\n",
    "cer = calculate_cer(reference_text, hypothesis_text)\n",
    "wer = calculate_wer(reference_text, hypothesis_text)\n",
    "\n",
    "print(f\"CER: {cer:.4f}, WER: {wer:.4f}\") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f1870ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           masked_input  \\\n",
      "0                         in being comparatively modern   \n",
      "1     the invention of movable metal letters in the ...   \n",
      "2     and it is worth mention in passing that as an ...   \n",
      "3     now as all books not primarily intended as [MA...   \n",
      "4     especially as no more time is occupied or cost...   \n",
      "...                                                 ...   \n",
      "2612  the secret service has been receiving full coo...   \n",
      "2613  even if the [MASK] [MASK] [MASK] resources of ...   \n",
      "2614  with the office of science and technology and ...   \n",
      "2615  made certain recommendations which it believes...   \n",
      "2616  as has been pointed out the commission has not...   \n",
      "\n",
      "                                          target_output  \n",
      "0                         in being comparatively modern  \n",
      "1     the invention of movable metal letters in the ...  \n",
      "2     and it is worth mention in passing that as an ...  \n",
      "3     now as all books not primarily intended as pic...  \n",
      "4     especially as no more time is occupied or cost...  \n",
      "...                                                 ...  \n",
      "2612  the secret service has been receiving full coo...  \n",
      "2613  even if the manpower and technological resourc...  \n",
      "2614  with the office of science and technology and ...  \n",
      "2615  made certain recommendations which it believes...  \n",
      "2616  as has been pointed out the commission has not...  \n",
      "\n",
      "[2617 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def mask_asr_errors(asr_sentence, correct_sentence):\n",
    "    asr_words = asr_sentence.split()\n",
    "    correct_words = correct_sentence.split()\n",
    "    \n",
    "    # Identify incorrect words using SequenceMatcher\n",
    "    matcher = difflib.SequenceMatcher(None, asr_words, correct_words)\n",
    "    \n",
    "    masked_sentence = asr_words.copy()\n",
    "    \n",
    "    for opcode, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if opcode in [\"replace\", \"insert\", \"delete\"]:\n",
    "            # Replace incorrect words with [MASK]\n",
    "            for idx in range(i1, i2):\n",
    "                masked_sentence[idx] = \"[MASK]\"\n",
    "    return \" \".join(masked_sentence), correct_sentence\n",
    "\n",
    "pred_df = pd.read_csv('pred_vs_ref.csv')\n",
    "pred_df = pred_df.dropna()\n",
    "processed_data = [mask_asr_errors(row[\"prediction\"], row[\"reference\"]) for _,row in pred_df.iterrows()]\n",
    "\n",
    "df_mask = pd.DataFrame(processed_data, columns=[\"masked_input\", \"target_output\"])\n",
    "print(df_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf393ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy ASR Output: I no their going to the park.\n",
      "Corrected Sentence: my i am theyre going to the park!!! your\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained ALBERT model and tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "model = AlbertForMaskedLM.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "def correct_asr_text(text):\n",
    "    \"\"\"\n",
    "    Takes noisy ASR text and corrects it using ALBERT's masked language modeling.\n",
    "    \"\"\"\n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "\n",
    "    # Decode predicted sentence\n",
    "    corrected_text = tokenizer.decode(torch.argmax(outputs, dim=2)[0], skip_special_tokens=True)\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "# Example: Noisy ASR Output\n",
    "noisy_text = \"I no their going to the park.\"\n",
    "corrected_text = correct_asr_text(noisy_text)\n",
    "\n",
    "print(\"Noisy ASR Output:\", noisy_text)\n",
    "print(\"Corrected Sentence:\", corrected_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb00f244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ASR-vrp)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
